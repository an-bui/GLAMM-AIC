---
title: "Model Selection with AIC"
author: "An Bui, Ana Sofia Guerra"
date: "7/23/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 0. Set up
```{r}
library(glmmTMB) # to get the Salamanders dataset
library(tidyverse) # to make your life easier
library(gt) # to render a table

data(Salamanders)

## OPTIONAL ##
library(remotes) # to install the dataset
remotes::install_github("allisonhorst/palmerpenguins") # example dataset
library(palmerpenguins)
```

Information taken from Kyle Edwards lectures [15](https://drive.google.com/file/d/0B5QCuGKrabF5ZzFyQ3VXaWNVRXM/view) and [16](https://drive.google.com/file/d/0B5QCuGKrabF5RzYxSEZuc0xoaVU/view) and Allison Horst's [ESM 206 class](https://github.com/allisonhorst/esm-206-lab-9).

## 1. What is model selection?

Model selection will help answer questions/address hypotheses like these:

```{r include = FALSE}
q_h <- tribble(
  ~Question, ~Hypothesis,
  "1. I have multiple models representing distinct hypotheses; which is best supported by the data?", "Different models represent different hypotheses.",
  "2. I have a set of predictors that are all hypothesized to be important for the response. Which are supported by the data? What is their relative importance? Should all be included in a single model, or should a smaller model of ‘significant’ predictors be used? How should such a smaller model be chosen?", "Different predictors represent different hypotheses (but a particular model could include one or more predictors)",
  "3. I have a large number of predictors that may or may not be important, and I want to do an exploratory analysis to see which are best supported by the
data. How do I construct model(s) to choose among them and quantify their importance?", "Not a hypothesis! Exploratory analysis / data mining / data dredging (not comparing a priori hypotheses, sifting for relationships)."
) %>% 
  gt()

q_h
```

### Why not throw all your model parameters in and see what happens?  
While tempting, this is a problem for a couple of reasons.  
1. Your predictors might be **correlated** meaning that predictors are dependent on each other, and their effects on model outputs are driven by how much they are related to each other.  
2. You may have **parameter uncertainty** meaning that as you add more predictors to your model, the less each predictor explains of the model variation. There is only _so much_ variation to be explained by predictors, and if you put more into your model, the variation attributed to any one predictor will get smaller with more predictors.

## 2. Akaike Information Criterion (AIC)

The AIC was created by [Hirotugu Akaike](https://www.ism.ac.jp/akaikememorial/index-e.html), and is a quantitative way to compare regression models. The AIC takes into account how well the model predicts the data _and_ increasing complexity. In general, the best model "compromises" between these two and strikes a balance between prediction and complexity (i.e. number of explanatory variables). 

We know that models only approximate reality, but we want to know which model approximates reality the _best_. With the AIC, we are quantifying the relative information loss of different models. The formula to do so is:
$$
AIC = -2*log(L(\hat{\theta}|Y)) + 2K
$$
where $L(\hat{\theta}|Y)$ is the likelihood of fitted parameters $\hat{\theta}$ given the data $Y$, and $K$ is the number of parameters in the model. The AIC number estimates the relative information lost in a particular model. The best model has the smallest AIC number. **NOTE:** having one AIC number (i.e. one model) is meaningless. The AIC works by comparing AIC numbers with others; therefore, one number doesn't mean anything on its own.  

### Bias-corrected AICc:
If your sample size is small (note: this is what Kyle Edwards writes... unsure what constitutes "small"), you can use the bias-corrected AICc (AIC correction) formulated by (Nariaki Sugiura)[https://doi.org/10.1080/03610927808827599]:
$$
AICc = -2*log(L(\hat{\theta}|Y)) + 2K(\frac{n}{n - K - 1})
$$
where $n$ is your sample size. As $n$ gets large relative to $K$, the second term gets closer to $2K$, as in the normal AIC. When $n$ is small relative to $K$, the second term is larger than $2K$, which penalizes more complex models more strongly.

## 3. How to use the AIC
It is... very easy to use the AIC. The `AIC()` function is in baseR. Hooray! As an example, we're going to use the `penguins` dataset from the Palmer Station LTER in Antarctica, coded up for our using pleasure by [Allison Horst](https://github.com/allisonhorst/palmerpenguins)!

```{r echo = FALSE}
data(penguins)
```

Let's start by visualizing the data.

```{r include = FALSE}
penguins_plot <- ggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = island)) +
  geom_point(size = 2) +
  scale_color_manual(values = c("#E29244", "#4CA49E", "#6B6D9F")) +
  theme_bw() +
  labs(x = "Flipper length (mm)", y = "Body mass (g)", title = "Palmer Penguins", color = "Island") +
  theme(legend.position = c(0.15, 0.75))

penguins_plot
```









