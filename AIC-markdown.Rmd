---
title: "Model Selection with AIC"
author: "An Bui, Ana Sofia Guerra"
date: "7/23/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 0. Set up
```{r}
library(glmmTMB) # to get the Salamanders dataset
library(tidyverse)
library(gt)

data(Salamanders)
```

Information taken from Kyle Edwards lectures [15](https://drive.google.com/file/d/0B5QCuGKrabF5ZzFyQ3VXaWNVRXM/view) and [16](https://drive.google.com/file/d/0B5QCuGKrabF5RzYxSEZuc0xoaVU/view) and Allison Horst's [ESM 206 class](https://github.com/allisonhorst/esm-206-lab-9).

## 1. What is model selection?

Model selection will help answer questions/address hypotheses like these:

```{r include = FALSE}
q_h <- tribble(
  ~Question, ~Hypothesis,
  "1. I have multiple models representing distinct hypotheses; which is best supported by the data?", "Different models represent different hypotheses.",
  "2. I have a set of predictors that are all hypothesized to be important for the response. Which are supported by the data? What is their relative importance? Should all be included in a single model, or should a smaller model of ‘significant’ predictors be used? How should such a smaller model be chosen?", "Different predictors represent different hypotheses (but a particular model could include one or more predictors)",
  "3. I have a large number of predictors that may or may not be important, and I want to do an exploratory analysis to see which are best supported by the
data. How do I construct model(s) to choose among them and quantify their importance?", "Not a hypothesis! Exploratory analysis / data mining / data dredging (not comparing a priori hypotheses, sifting for relationships)."
) %>% 
  gt()

q_h
```

### Why not throw all your model parameters in and see what happens?  
While tempting, this is a problem for a couple of reasons.  
1. Your predictors might be **correlated** meaning that predictors are dependent on each other, and their effects on model outputs are driven by how much they are related to each other.  
2. You may have **parameter uncertainty** meaning that as you add more predictors to your model, the less each predictor explains of the model variation. There is only _so much_ variation to be explained by predictors, and if you put more into your model, the variation attributed to any one predictor will get smaller with more predictors.

## 2. Akaike Information Criterion (AIC)

The AIC was created by [Hirotugu Akaike](https://www.ism.ac.jp/akaikememorial/index-e.html), and is a quantitative way to compare regression models. The AIC takes into account how well the model predicts the data _and_ increasing complexity. In general, the best model "compromises" between these two and strikes a balance between prediction and complexity (i.e. number of explanatory variables). **The best model is given the _lowest_ AIC value.**  




